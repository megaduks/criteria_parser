{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Contains functions to read and parse information from the Chia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[entities]\r\n",
      "!CONCEPTS\r\n",
      "\tScope\r\n",
      "\tPerson\r\n",
      "\tCondition\r\n",
      "\tDrug\r\n",
      "\tObservation\r\n",
      "\tMeasurement\r\n",
      "\tProcedure\r\n",
      "\tDevice\r\n",
      "\tVisit\r\n",
      "!ANNOTATION\r\n",
      "\tNegation\r\n",
      "\tQualifier\r\n",
      "\tTemporal\r\n",
      "\tValue\r\n",
      "\tMultiplier\r\n",
      "\tReference_point\r\n",
      "\tLine\r\n",
      "\tMood\r\n",
      "!ERROR\r\n",
      "\tNon-query-able\r\n",
      "\tPost-eligibility\r\n",
      "\tPregnancy_considerations\r\n",
      "\tCompeting_trial\r\n",
      "\tInformed_consent\r\n",
      "\tIntoxication_considerations\r\n",
      "\tNon-representable\r\n",
      "\r\n",
      "[events]\r\n",
      "\r\n",
      "[relations]\r\n",
      "h-OR   Arg1:<ENTITY>, Arg2:<ENTITY>, <REL-TYPE>:symmetric-transitive\r\n",
      "v-AND Arg1:<ANY>, Arg2:<ANY>\r\n",
      "v-OR Arg1:<ANY>, Arg2:<ANY>\r\n",
      "multi Arg1:<ANY>, Arg2:<ANY>\r\n",
      "<OVERLAP>\tArg1:<ANY>, Arg2:<ANY>, <OVL-TYPE>:<ANY>\r\n",
      "\r\n",
      "[attributes]\r\n",
      "Optional   Arg:<ANY>\r\n",
      " "
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!cat data/annotation.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_chia() -> pd.DataFrame:\n",
    "    \"\"\"Exports Chia annotated dataset as a Pandas dataframe\"\"\"\n",
    "    \n",
    "    _lst = []\n",
    "    \n",
    "    ent_map = {\n",
    "        \"drugs\": \"Drug\", \n",
    "        \"persons\": \"Person\", \n",
    "        \"procedures\": \"Proceure\", \n",
    "        \"conditions\": \"Condition\",\n",
    "        \"devices\": \"Device\",\n",
    "        \"visits\": \"Visit\",\n",
    "        \"scopes\": \"Scope\",\n",
    "        \"observations\": \"Observation\",\n",
    "        \"measurements\": \"Measurement\",\n",
    "    }\n",
    "    \n",
    "    for mode in [\"_inc\", \"_exc\"]:\n",
    "        \n",
    "        criteria_files = Path(\"data\").glob(f\"*{mode}.txt\")\n",
    "\n",
    "        for f in criteria_files:\n",
    "            clinical_trial_no = str(f).lstrip(\"data/\").rstrip(f\"{mode}.txt\")\n",
    "\n",
    "            with open(f, \"rt\") as f:\n",
    "                criteria = \" \".join(f.read().splitlines())\n",
    "                \n",
    "            _rec = {}\n",
    "\n",
    "            _rec[\"ct_no\"] = clinical_trial_no\n",
    "            _rec[\"criteria\"] = criteria\n",
    "            _rec[\"mode\"] = \"inclusion\" if mode == \"_inc\" else \"exclusion\"\n",
    "\n",
    "            for e in ent_map:\n",
    "                ents = extract_entities(clinical_trial_no, mode, ent_map[e])\n",
    "                _rec[e] = ents if ents else None\n",
    "\n",
    "            _lst.append(_rec)\n",
    "        \n",
    "    return pd.DataFrame(_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def extract_entities(ct: str, mode: str, e: str) -> List:\n",
    "    entities = []\n",
    "    \n",
    "    with open(f\"data/{ct}{mode}.ann\", \"rt\") as f:\n",
    "        data = f.read().splitlines()\n",
    "        \n",
    "    for row in data:\n",
    "        if e in row:\n",
    "            entities.append(\" \".join(row.split()[4:]))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "df = load_chia()\n",
    "nrow, ncol = df.shape\n",
    "\n",
    "test_eq(nrow, 2000)\n",
    "test_eq(ncol, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Set\n",
    "\n",
    "def jaccard_score(a: Set, b: Set, mode: str=\"strict\") -> float:\n",
    "    \"\"\"Computes different versions of the Jaccard score depending on the requested mode\n",
    "    \n",
    "    strict: |a & b| / |a + b|\n",
    "    relaxed: |a & b| / min{|a|,|b|}\n",
    "    left: |a & b| / |a|\n",
    "    right: |a & b| / |b|\n",
    "    \"\"\"\n",
    "    \n",
    "    if (not a) or (not b):\n",
    "        return 0.\n",
    "    \n",
    "    if mode == \"strict\":\n",
    "        return len(a.intersection(b)) / len(a.union(b))\n",
    "    elif mode == \"relaxed\":\n",
    "        return len(a.intersection(b)) / min(len(a), len(b))\n",
    "    elif mode == \"left\":\n",
    "        return len(a.intersection(b)) / len(a)\n",
    "    elif mode == \"right\":\n",
    "        return len(a.intersection(b)) / len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from fastcore.test import *\n",
    "\n",
    "left = set([\"ala\", \"ma\", \"kota\"])\n",
    "right = set([\"ola\", \"ma\", \"psa\", \"i\", \"papugÄ™\"])\n",
    "\n",
    "test_eq(jaccard_score(left, right, mode=\"strict\"), 1/7)\n",
    "test_eq(jaccard_score(left, right, mode=\"relaxed\"), 1/3)\n",
    "test_eq(jaccard_score(left, right, mode=\"left\"), 1/3)\n",
    "test_eq(jaccard_score(left, right, mode=\"right\"), 1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "def entity_coverage_score(\n",
    "    ents_true: List[str], \n",
    "    ents_pred: List[str], \n",
    "    mode: str, \n",
    "    threshold: float=0.) -> Tuple[float, float]:\n",
    "    \"\"\"Compute the compound metric of entity coverage in eligibility criteria\n",
    "    \n",
    "    Args:\n",
    "        ents_true: entities from Chia annotations\n",
    "        ents_pred: predicted entities\n",
    "        mode: which version of Jaccard coefficient to use\n",
    "        threshold: only matches with Jaccard coefficient above the threshold will count as non-zero\n",
    "    \n",
    "    \n",
    "    For each entity in a criterion, find the predicted entity which maximizes the Jaccard score and\n",
    "    return the average Jaccard score for matched entities and the percentage of entites for which\n",
    "    any matching has been found\n",
    "    \"\"\"\n",
    "    \n",
    "    if not ents_true:\n",
    "        ents_true = [] # make sure that ents_true is iterable\n",
    "    \n",
    "    if not ents_pred:\n",
    "        return (0., 0.) # max() cannot operate on empty sequence\n",
    "\n",
    "    scores = [\n",
    "        max([jaccard_score(set(e_true.split()), set(e_pred.split()), mode=mode) for e_pred in ents_pred])\n",
    "        for e_true\n",
    "        in ents_true\n",
    "    ]\n",
    "    non_zero_scores = [s for s in scores if s > threshold]\n",
    "    \n",
    "    if not non_zero_scores:\n",
    "        return (0., 0.)\n",
    "        \n",
    "    return (\n",
    "        sum(non_zero_scores) / len(non_zero_scores), # average Jaccard score of matched entities\n",
    "        len(non_zero_scores) / len(scores), # percentage of matched entities\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# test basic usage of entity_coverage function\n",
    "ents_true = ['adult', 'no alcohol substance abuse', 'cardiovascular disease', 'elevated cholesterol']\n",
    "ents_pred = ['adult man or woman', 'no alcohol usage during last year', 'high blood pressure']\n",
    "\n",
    "test_eq(entity_coverage_score(ents_true, ents_pred, mode=\"strict\"), (0.25, 0.5))\n",
    "test_eq(entity_coverage_score(ents_true, ents_pred, mode=\"relaxed\"), (0.75, 0.5))\n",
    "\n",
    "# test usage of entity_coverage when there are no true entities\n",
    "ents_true = []\n",
    "ents_pred = ['adult man or woman', 'no alcohol usage during last year', 'high blood pressure']\n",
    "\n",
    "test_eq(entity_coverage_score(ents_true, ents_pred, mode=\"strict\"), (0., 0.))\n",
    "\n",
    "ents_true = None\n",
    "ents_pred = ['adult man or woman', 'no alcohol usage during last year', 'high blood pressure']\n",
    "\n",
    "test_eq(entity_coverage_score(ents_true, ents_pred, mode=\"strict\"), (0., 0.))\n",
    "\n",
    "# test usage of entity_coverage when there are no true entities\n",
    "ents_true = ['adult', 'no alcohol substance abuse', 'cardiovascular disease', 'elevated cholesterol']\n",
    "ents_pred = []\n",
    "\n",
    "test_eq(entity_coverage_score(ents_true, ents_pred, mode=\"strict\"), (0., 0.))\n",
    "\n",
    "ents_true = ['adult', 'no alcohol substance abuse', 'cardiovascular disease', 'elevated cholesterol']\n",
    "ents_pred = None\n",
    "\n",
    "test_eq(entity_coverage_score(ents_true, ents_pred, mode=\"strict\"), (0., 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_annotations(\n",
    "    entity: str, n: int = None, random: bool = False\n",
    ") -> List[Tuple[int, str, str]]:\n",
    "    df = load_chia()\n",
    "\n",
    "    if random:\n",
    "        result = (\n",
    "            df[~df[entity].isna()][[\"ct_no\", \"criteria\", entity]][:n]\n",
    "            .sample(frac=1.0)\n",
    "            .to_records(index=False)\n",
    "            .tolist()\n",
    "        )\n",
    "    else:\n",
    "        result = (\n",
    "            df[~df[entity].isna()][[\"ct_no\", \"criteria\", entity]][:n]\n",
    "            .to_records(index=False)\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "test_eq(len(get_annotations(\"drugs\", n=100)), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biogpt_prompt_ner(_text: str, entity: str) -> List[str]:\n",
    "    try:\n",
    "        src_tokens = m.encode(_text)\n",
    "        original_len = len(m.decode(src_tokens))\n",
    "        generate = m.generate([src_tokens])[0]\n",
    "        output = m.decode(generate[0][\"tokens\"])[original_len:]\n",
    "\n",
    "        lst_output = (\n",
    "            output.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            .replace(f\"{entity}\", \"\")\n",
    "            .split()\n",
    "        )\n",
    "\n",
    "        return list(set(lst_output))\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_ners(x: str) -> List[str]:\n",
    "    return (\n",
    "        x.replace(\"[\", \"\")\n",
    "        .replace(\"]\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\", \", \",\")\n",
    "        .split(\",\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def biogpt_prompt_ner(prompt: str, entity: str, model: object) -> List[str]:\n",
    "    \"\"\"Applies the model to a prompt\"\"\"\n",
    "    try:\n",
    "        src_tokens = model.encode(prompt)\n",
    "        original_len = len(model.decode(src_tokens))\n",
    "        generate = model.generate([src_tokens])[0]\n",
    "        output = model.decode(generate[0][\"tokens\"])[original_len:]\n",
    "\n",
    "        return output\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def prompt_score(ents_true: List[str], ents_pred: List[str]) -> Dict:\n",
    "    \"\"\"Computes aggregates (means and standard deviations) of all types of Jaccard coeff scores\"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    modes = [\"strict\", \"relaxed\", \"left\", \"right\"]\n",
    "    \n",
    "    for mode in modes:\n",
    "        _lst = [entity_coverage(e_true, e_pred, mode=mode) for e_true, e_pred in zip(ents_true, ents_pred)]\n",
    "        jaccard_scores, pct_scores = zip(*_lst)\n",
    "        result[mode] = (\n",
    "            statistics.mean(jaccard_scores),\n",
    "            statistics.stdev(jaccard_scores),\n",
    "            statistics.mean(pct_scores),\n",
    "            statistics.stdev(pct_scores)\n",
    "        )\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def fit_prompt(\n",
    "    examples: List[Tuple[int, str,str]],\n",
    "    entity: str,\n",
    "    model: object,\n",
    "    prompt_fun: Callable,\n",
    "    deprompt_fun: Callable,\n",
    "    n_shots: int = 1,\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"Applies prompting to all input examples and returns predicted entities\"\"\"\n",
    "\n",
    "    ids, criteria, ents_true = zip(*examples)\n",
    "\n",
    "    ents_pred = [\n",
    "        deprompt_fun(biogpt_prompt_ner(prompt_fun(criterion, examples, entity, n_shots), entity, m), entity)\n",
    "        for criterion in tqdm(criteria)\n",
    "    ]\n",
    "    \n",
    "    return ents_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
